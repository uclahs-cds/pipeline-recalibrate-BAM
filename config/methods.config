import nextflow.util.SysHelper
includeConfig "${projectDir}/external/pipeline-Nextflow-config/config/csv/csv_parser.config"
includeConfig "${projectDir}/external/pipeline-Nextflow-config/config/bam/bam_parser.config"
includeConfig "${projectDir}/external/pipeline-Nextflow-config/config/methods/common_methods.config"
includeConfig "${projectDir}/external/pipeline-Nextflow-config/config/schema/schema.config"
includeConfig "${projectDir}/external/pipeline-Nextflow-config/config/resource_handler/resource_handler.config"
includeConfig "${projectDir}/external/pipeline-Nextflow-config/config/store_object_as_json/store_object_as_json.config"

methods {

    set_log_output_dir = {
        def tz = TimeZone.getTimeZone("UTC")
        def date = new Date().format("yyyyMMdd'T'HHmmss'Z'", tz)

        params.log_output_dir = "${params.output_dir}/${manifest.name}-${manifest.version}/${params.patient_id}/log-${manifest.name}-${manifest.version}-${date}"
    }

    set_output_dir = {
        params.output_dir_base = "${params.output_dir}/${manifest.name}-${manifest.version}/${params.patient_id}/${params.docker_image_gatk.split("/")[1].replace(':', '-').toUpperCase()}"
    }

    set_pipeline_logs = {
        trace.enabled = true
        trace.file = "${params.log_output_dir}/nextflow-log/trace.txt"

        timeline.enabled = true
        timeline.file = "${params.log_output_dir}/nextflow-log/timeline.html"

        report.enabled = true
        report.file = "${params.log_output_dir}/nextflow-log/report.html"
    }

    set_process = {
        process.cache = params.cache_intermediate_pipeline_steps
    }

    detect_mode = {
        // Detect whether job is for targeted sample
        params.is_targeted = (params.containsKey('intervals') && params.intervals) ? true : false
    }

    set_ids_from_bams = {
        params.samples_to_process = [] as Set
        params.input.BAM.each { k, v ->
            v.each { bam_path ->
                def bam_header = bam_parser.parse_bam_header(bam_path)
                def sm_tags = bam_header['read_group'].collect{ it['SM'] }.unique()

                if (sm_tags.size() != 1) {
                    throw new Exception("${bam_path} contains multiple samples! Please run pipeline with single sample BAMs.")
                }

                params.samples_to_process.add(['id': sm_tags[0], 'sample_id': sm_tags[0], 'path': bam_path, 'bam': bam_path, 'sample_type': k])
            }
        }
    }

    set_allocation = {
        def node_memory_GB = SysHelper.getAvailMemory().toGiga()

        def num_samples = params.samples_to_process.size()

        // Divide memory per sample, with a minimum of 20% of total memory
        def mem_to_allocate = Math.max(node_memory_GB * 0.2, (node_memory_GB * 0.9) / num_samples)

        Map custom_allocations = [:]

        def processes_to_parallelize_per_sample = [
            'run_MergeSamFiles_Picard',
            'deduplicate_records_SAMtools'
        ]

        def processes_to_parallelize_per_half_sample = [
            'run_DepthOfCoverage_GATK'
        ]

        processes_to_parallelize_per_sample.each { p ->
            custom_allocations[p] = [:]
            custom_allocations[p]['memory'] = "${ mem_to_allocate } GB" as nextflow.util.MemoryUnit
        }

        processes_to_parallelize_per_half_sample.each { p ->
            custom_allocations[p] = [:]
            custom_allocations[p]['memory'] = "${ mem_to_allocate / 2 } GB" as nextflow.util.MemoryUnit
        }

        // Set base and node-specific allocations
        resource_handler.handle_resources("${projectDir}/config/resources.json", custom_allocations)
    }

    verify_input_deletion = {
        if (!params.containsKey('metapipeline_final_output_dir')) {
            params.metapipeline_delete_input_bams = false
        }
    }

    set_recal_tables = {
        params.use_recal_tables = params.input.containsKey('recalibration_table')

        if (!params.use_recal_tables) {
            params.input['recalibration_table'] = ["${params.work_dir}/NO_FILE.grp"]
        }
    }

    validate_selection = {
        if (!(params.run_bqsr || params.run_indelrealignment)) {
            throw new Exception("At least one of BQSR and IndelRealignment must be enabled using params: `run_bqsr` and `run_indelrealignment`.")
        }
    }

    setup = {
        methods.set_env()
        schema.load_custom_types("${projectDir}/config/custom_schema_types.config")
        schema.validate()
        methods.set_ids_from_bams()
        methods.set_allocation()
        methods.set_log_output_dir()
        methods.set_output_dir()
        methods.set_pipeline_logs()
        methods.set_process()
        methods.detect_mode()
        methods.setup_docker_cpus()
        methods.verify_input_deletion()
        methods.set_recal_tables()
        methods.setup_process_afterscript()

        json_extractor.store_object_as_json(
            params,
            new File("${params.log_output_dir}/nextflow-log/params.json")
        )
    }
}
